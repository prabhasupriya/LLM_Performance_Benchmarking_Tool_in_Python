LLM Performance Benchmarking Tool

A CLI-based benchmarking framework designed to evaluate and compare open-source Large Language Models (LLMs) based on inference latency, throughput, memory consumption, and output quality.
The tool helps understand performanceâ€“resource trade-offs across models of different sizes.

ğŸ“Œ Project Objectives

To measure inference performance of LLMs in a standardized manner

To compare lightweight vs large models

To analyze latency, throughput, and memory usage

To generate visual and tabular benchmark reports

To support CPU and GPU environments

âš™ï¸ Key Features

âœ… Config-driven benchmarking via YAML
âœ… HuggingFace Transformer model support
âœ… CPU & GPU inference benchmarking
âœ… RAM and GPU memory monitoring
âœ… Latency & throughput measurement
âœ… Vocabulary diversity as output quality metric
âœ… Automatic CSV report generation
âœ… Visualization plots for performance comparison

ğŸ§  Models Evaluated
Model	Size	Environment
sshleifer/tiny-gpt2	~6M params	Local CPU
bigscience/bloom-560m	560M params	Local CPU

 âš ï¸ Optional / Future Models
The following models were considered but not benchmarked in this project due to hardware constraints:
- EleutherAI/gpt-neo-1.3B (requires High-RAM system)
- DistilGPT2 (medium, optional)
- Falcon-7B (large, Colab GPU)
- Pythia-2.8B (large, Colab GPU)



## Project Structure
LLM-Benchmarking-Tool/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prompts.jsonl
â”‚   â””â”€â”€ again_quiz.jsonl
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ data_loader.py     # Loads prompt datasets
â”‚   â”œâ”€â”€ loader.py          # Loads HuggingFace models
â”‚   â”œâ”€â”€ metrics.py         # Latency, throughput, diversity
â”‚   â”œâ”€â”€ monitor.py         # RAM & GPU usage tracking
â”‚   â””â”€â”€ visualizer.py      # Performance plots
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ metrics.csv
â”‚   â”œâ”€â”€ latency_plot.png
â”‚   â””â”€â”€ memory_plot.png
â”‚
â”œâ”€â”€ config.yaml
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## Output Files
File	Description
metrics.csv	Detailed benchmark metrics per prompt
latency_plot.png	Average inference latency comparison
memory_plot.png	Average RAM/GPU memory usage
Console Summary	Mean metrics per model
## Metrics Explained
ğŸ”¹ Latency (seconds)

Time taken to generate output for a prompt.

ğŸ”¹ Throughput (tokens/sec)

Number of tokens generated per second.

ğŸ”¹ RAM Usage (MB)

Difference in memory before and after inference.

ğŸ”¹ GPU Memory (MB)

Measured via NVIDIA NVML (if GPU available).

ğŸ”¹ Vocabulary Diversity

Ratio of unique tokens to total tokens in output.

## Sample Observation

BLOOM-560M consumes significantly more memory than Tiny-GPT2, confirming the expected trade-off between model size and resource usage.
Tiny-GPT2 offers fast inference with minimal memory, while BLOOM delivers richer outputs at higher computational cost.

## Setup Instructions
1ï¸âƒ£ Create Virtual Environment
python -m venv venv

2ï¸âƒ£ Activate Environment (Windows)
venv\Scripts\activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

## Running the Benchmark
python main.py --config config.yaml

## Configuration (config.yaml)
models:
  - sshleifer/tiny-gpt2
  - bigscience/bloom-560m

dataset_path: data/prompts.jsonl

generation:
  max_new_tokens: 50

### System Compatibility
Environment	Supported
Windows	YES
Linux	YES
CPU	YES
NVIDIA GPU	YES
Google Colab	YES
### Future Enhancements

Batch inference support

Energy consumption tracking

Per-token latency analysis

Web-based dashboard

BLEU / ROUGE evaluation

### Conclusion

This tool provides a scalable, modular, and reproducible benchmarking pipeline for evaluating LLMs.
It enables informed decisions when choosing models based on performance vs resource constraints, making it valuable for both academic research and practical deployment scenarios.