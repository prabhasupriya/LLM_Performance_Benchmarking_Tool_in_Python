# LLM Performance Benchmarking Tool

A CLI-based tool to benchmark open-source Large Language Models (LLMs) on **inference latency, throughput, memory usage, and output quality**.


## âš¡ Tool Features

âœ… Config-driven CLI interface  
âœ… HuggingFace model evaluation  
âœ… CPU + GPU benchmarking  
âœ… 8-bit memory optimization support  
âœ… Memory & latency profiling  
âœ… Visualization charts (latency & memory)  
âœ… CSV metrics export  



## ðŸ§  Models Tested

- DistilGPT2  
- Pythia-2.8B (tested on Colab GPU)  
- Falcon-7B (tested on Colab GPU)  
- Tiny-GPT2, Bloom-560M, GPT-Neo-1.3B (optional/custom)



## ðŸ“Š Outputs

| File | Purpose |
|------|---------|
| `results.csv` | Benchmark summary with latency & memory metrics |
| `latency_plot.png` | Latency comparison chart |
| `memory_plot.png` | GPU/CPU memory usage chart |
| `model_responses.json` | Prompt-response pairs from all models |



## ðŸš€ How to Run

```bash


# Install dependencies
pip install -r requirements.txt

# Run benchmark
python main.py --config config.yaml
